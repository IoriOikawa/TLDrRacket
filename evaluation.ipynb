{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d9eec2-65aa-4376-95fe-33df539afdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45982/694096650.py:51: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    with open(\"./.env\", \"r\") as mykey:        \n",
    "        os.environ[\"GOOGLE_API_KEY\"] = mykey.read().strip()\n",
    "\n",
    "# from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "# from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Embedding Function: Used when creating the DB, or making a query.\n",
    "def get_embedding_function():\n",
    "    # Bedrock Embeddings for AWS Deploy\n",
    "    # embeddings = BedrockEmbeddings(\n",
    "    #    credentials_profile_name=\"default\", region_name=\"us-east-1\"\n",
    "    #)\n",
    "    # Ollama Embeddings for Local Run\n",
    "    # Install and 'ollama pull llama2|mistral' to deploy.\n",
    "    # Use 'ollama serve' for restful API\n",
    "    # embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    # \n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "    return embeddings\n",
    "\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain_community.llms.ollama import Ollama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Prepare the DB.\n",
    "embedding_function = get_embedding_function()\n",
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\" : 5})\n",
    "# Generate query with the info augmented prompt.\n",
    "# context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "# prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "# print(prompt)\n",
    "\n",
    "# model = Ollama(model=\"mistral\")\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
    "# response_text = model.invoke(prompt)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "def run(user_message: str):\n",
    "    response = rag_chain.invoke(user_message)\n",
    "    \n",
    "    contexts = db.similarity_search_with_score(user_message, k=5)\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in contexts]\n",
    "    \n",
    "    return {\"answer\": response, \"contexts\": sources}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5367d88-15f9-4c9d-a1b7-0bbd9789c193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|█████████████████████████████████████████████| 2/2 [00:03<00:00,  1.52s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data_samples = {\n",
    "    'question': [\n",
    "        'How to write a lambda in Racket?', \n",
    "        'What are First Order Functions, Higher Order Functions, and First Class Functions?'\n",
    "    ],\n",
    "    'answer': [],\n",
    "    'contexts': [],\n",
    "    'ground_truth': [\n",
    "        \"\"\"\n",
    "         In Racket, you can use a lambda expression to produce a function directly.\n",
    "         The lambda form is followed by identifiers for the function’s arguments, and then the function’s body expressions: ( lambda ( xidy* ) xexpry+ ).\n",
    "         A lambda expression can also have the form (lambda rest-id body ...+)\n",
    "         That is, a lambda expression can have a single rest-id that is not surrounded by parentheses.\n",
    "         The resulting function accepts any number of arguments, and the arguments are put into a list bound to rest-id.\"\"\",\n",
    "        \"\"\"\n",
    "         First order: functions are not real values. They cannot be used or returned as values by other functions.\n",
    "         This means that they cannot be stored in data structures. This is what most “conventional” languages used to have in the past. (You will be implementing such a language in homework 4.)\n",
    "         An example of such a language is the Beginner Student language that is used in HtDP, where the language is intentionally first-order to help students write correct code (at the early stages where using a function as a value is usually an error).\n",
    "         It’s hard to find practical modern languages that fall in this category.\n",
    "         Higher order: functions can receive and return other functions as values. This is what you get with C and modern Fortran.\n",
    "         First class: functions are values with all the rights of other values.\n",
    "         In particular, they can be supplied to other functions, returned from functions, stored in data structures, and new functions can be created at run-time. (And most modern languages have first class functions.)\"\"\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for question in tqdm(data_samples['question']):\n",
    "    result = run(question)\n",
    "    data_samples['answer'].append(result['answer'])\n",
    "    data_samples['contexts'].append(result['contexts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "945ccef2-a451-4021-99a8-c3e286370d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ceb1d3fa5642a880b1e4ca7fbc8b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[2]: TypeError(generate_content() got an unexpected keyword argument 'temperature')\n",
      "Exception raised in Job[0]: TypeError(generate_content() got an unexpected keyword argument 'temperature')\n",
      "Exception raised in Job[1]: TypeError(generate_content() got an unexpected keyword argument 'temperature')\n",
      "Exception raised in Job[3]: TimeoutError()\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(data_samples)\n\u001b[1;32m     69\u001b[0m g \u001b[38;5;241m=\u001b[39m LangchainLLMWrapper(model, is_finished_parser\u001b[38;5;241m=\u001b[39mgemini_is_finished_parser)\n\u001b[0;32m---> 70\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_correctness\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m df \u001b[38;5;241m=\u001b[39m score\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     72\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Courseworks/DS5983/TLDrRacket/lib/python3.9/site-packages/ragas/_analytics.py:205\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m    204\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 205\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Courseworks/DS5983/TLDrRacket/lib/python3.9/site-packages/ragas/evaluation.py:333\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;66;03m# evalution run was successful\u001b[39;00m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;66;03m# now lets process the results\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     cost_cb \u001b[38;5;241m=\u001b[39m ragas_callbacks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcost_cb\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcost_cb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ragas_callbacks \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mEvaluationResult\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbinary_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcost_cb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m            \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUnion\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCostCallbackHandler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcost_cb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mragas_traces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[1;32m    345\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39mscores})\n",
      "File \u001b[0;32m<string>:10\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, scores, dataset, binary_columns, cost_cb, traces, ragas_traces, run_id)\u001b[0m\n",
      "File \u001b[0;32m~/Courseworks/DS5983/TLDrRacket/lib/python3.9/site-packages/ragas/dataset_schema.py:410\u001b[0m, in \u001b[0;36mEvaluationResult.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# parse the traces\u001b[39;00m\n\u001b[1;32m    409\u001b[0m run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_id) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraces \u001b[38;5;241m=\u001b[39m \u001b[43mparse_run_traces\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mragas_traces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Courseworks/DS5983/TLDrRacket/lib/python3.9/site-packages/ragas/callbacks.py:167\u001b[0m, in \u001b[0;36mparse_run_traces\u001b[0;34m(traces, parent_run_id)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, prompt_uuid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(metric_trace\u001b[38;5;241m.\u001b[39mchildren):\n\u001b[1;32m    164\u001b[0m         prompt_trace \u001b[38;5;241m=\u001b[39m traces[prompt_uuid]\n\u001b[1;32m    165\u001b[0m         prompt_traces[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_trace\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_trace\u001b[38;5;241m.\u001b[39minputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[0;32m--> 167\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mprompt_trace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    168\u001b[0m         }\n\u001b[1;32m    169\u001b[0m     metric_traces[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_trace\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt_traces\n\u001b[1;32m    170\u001b[0m parased_traces\u001b[38;5;241m.\u001b[39mappend(metric_traces)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset \n",
    "import os\n",
    "from ragas import evaluate\n",
    "from ragas.llms import BaseRagasLLM, LangchainLLMWrapper\n",
    "from ragas.metrics import faithfulness, answer_correctness\n",
    "from langchain.schema import LLMResult, Generation\n",
    "\n",
    "class Gemini(BaseRagasLLM):\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt,\n",
    "        n: int = 1,\n",
    "        temperature: float = 1e-8,\n",
    "        stop = None,\n",
    "        callbacks = None,\n",
    "    ):\n",
    "        print(f'#################################Asking gemini len={len(prompt.to_string())}')\n",
    "        # print(prompt)\n",
    "        ai_message = model.invoke(prompt)\n",
    "        print(f'#################################Got response from gemini len={len(ai_message.content)}')\n",
    "        # print(ai_message.content);\n",
    "        generation = Generation(text=ai_message.content)\n",
    "        return LLMResult(generations=[[generation]])\n",
    "        \n",
    "    async def agenerate_text(\n",
    "        self,\n",
    "        prompt,\n",
    "        n: int = 1,\n",
    "        temperature = None,\n",
    "        stop = None,\n",
    "        callbacks = None,\n",
    "    ):\n",
    "        return self.generate_text(prompt,n,temperature,stop,callbacks)\n",
    "\n",
    "# Create a custom is_finished_parser to capture Gemini generation completion signals\n",
    "def gemini_is_finished_parser(response: LLMResult) -> bool:\n",
    "    is_finished_list = []\n",
    "    for g in response.flatten():\n",
    "        resp = g.generations[0][0]\n",
    "\n",
    "        # Check generation_info first\n",
    "        if resp.generation_info is not None:\n",
    "            finish_reason = resp.generation_info.get(\"finish_reason\")\n",
    "            if finish_reason is not None:\n",
    "                is_finished_list.append(\n",
    "                    finish_reason in [\"STOP\", \"MAX_TOKENS\"]\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        # Check response_metadata as fallback\n",
    "        if isinstance(resp, ChatGeneration) and resp.message is not None:\n",
    "            metadata = resp.message.response_metadata\n",
    "            if metadata.get(\"finish_reason\"):\n",
    "                is_finished_list.append(\n",
    "                    metadata[\"finish_reason\"] in [\"STOP\", \"MAX_TOKENS\"]\n",
    "                )\n",
    "            elif metadata.get(\"stop_reason\"):\n",
    "                is_finished_list.append(\n",
    "                    metadata[\"stop_reason\"] in [\"STOP\", \"MAX_TOKENS\"] \n",
    "                )\n",
    "\n",
    "        # If no finish reason found, default to True\n",
    "        if not is_finished_list:\n",
    "            is_finished_list.append(True)\n",
    "\n",
    "    return all(is_finished_list)\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "g = LangchainLLMWrapper(model, is_finished_parser=gemini_is_finished_parser)\n",
    "score = evaluate(dataset, metrics=[faithfulness, answer_correctness], llm=g, embeddings=embedding_function)\n",
    "df = score.to_pandas()\n",
    "df.to_csv('score.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cb05fc-df3e-4b0e-8ea0-f8568fde1c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
